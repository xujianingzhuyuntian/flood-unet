{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm \n",
    "from IPython.display import  clear_output\n",
    "from torchvision import transforms\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set training and testing dataset paths\n",
    "train_folder_paths = [f\"Datasets/Breach{i}/{j}\" for i in [1, 3, 5, 7, 9] for j in [1, 2, 3]]\n",
    "test_folder_paths = [f\"Datasets/Breach{i}/{j}\" for i in [2] for j in [1, 2, 3]]\n",
    "\n",
    "Transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "])\n",
    "\n",
    "ROLL_LENGTH = 4  # number of steps for rolling prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Datasets/Mask.dat not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load and display mask file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mask_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDatasets/Mask.dat\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask_data[:, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m474\u001b[39m, \u001b[38;5;241m320\u001b[39m)\n\u001b[0;32m      4\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([mask, np\u001b[38;5;241m.\u001b[39mfull((\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m320\u001b[39m), \u001b[38;5;241m0\u001b[39m)])\n",
      "File \u001b[1;32mc:\\Users\\win10\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:1373\u001b[0m, in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1371\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1373\u001b[0m arr \u001b[38;5;241m=\u001b[39m _read(fname, dtype\u001b[38;5;241m=\u001b[39mdtype, comment\u001b[38;5;241m=\u001b[39mcomment, delimiter\u001b[38;5;241m=\u001b[39mdelimiter,\n\u001b[0;32m   1374\u001b[0m             converters\u001b[38;5;241m=\u001b[39mconverters, skiplines\u001b[38;5;241m=\u001b[39mskiprows, usecols\u001b[38;5;241m=\u001b[39musecols,\n\u001b[0;32m   1375\u001b[0m             unpack\u001b[38;5;241m=\u001b[39munpack, ndmin\u001b[38;5;241m=\u001b[39mndmin, encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   1376\u001b[0m             max_rows\u001b[38;5;241m=\u001b[39mmax_rows, quote\u001b[38;5;241m=\u001b[39mquotechar)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32mc:\\Users\\win10\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:992\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[0;32m    990\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 992\u001b[0m     fh \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m_datasource\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    994\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\win10\\anaconda3\\Lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mopen(path, mode, encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n",
      "File \u001b[1;32mc:\\Users\\win10\\anaconda3\\Lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Datasets/Mask.dat not found."
     ]
    }
   ],
   "source": [
    "# Load and display mask file\n",
    "mask_data = np.loadtxt('Datasets/Mask.dat')\n",
    "mask = mask_data[:, 2].reshape(474, 320)\n",
    "mask = np.vstack([mask, np.full((6, 320), 0)])\n",
    "plt.imshow(np.flipud(mask), cmap='jet', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folder(folder_path, shape=(474, 320), length=24):\n",
    "    # Read .dat files\n",
    "    file_list = sorted(f for f in os.listdir(folder_path) if f.endswith('.dat'))\n",
    "    data_list = []\n",
    "    for file_name in file_list[0:length]:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            data = np.loadtxt(file_path)\n",
    "            data_list.append(data)\n",
    "        except ValueError as e:\n",
    "            print(f\"Cannot read file {file_name}: {e}\")\n",
    "\n",
    "    if not data_list:\n",
    "        raise ValueError(\"No valid data found in folder!\")\n",
    "\n",
    "    dx = dy = dt = 1  # unit spacing and time\n",
    "    all_variables = []\n",
    "\n",
    "    # Extract elevation from first file\n",
    "    elevation = data_list[0][:, 2].reshape(shape)\n",
    "    elevation = np.vstack([elevation, np.full((6, 320), 35)])\n",
    "    elevation_new = elevation * mask\n",
    "    elevation_new[elevation_new == 0] = 35\n",
    "    all_variables.append(elevation_new)\n",
    "\n",
    "    # Initialize depth at t0 with zeros\n",
    "    depth_init_0 = np.full((480, 320), 0)\n",
    "    all_variables.append(depth_init_0)\n",
    "\n",
    "    # Compute water depth from water level and elevation\n",
    "    for data in data_list:\n",
    "        water_level = data[:, 3] - elevation[:474].flatten()\n",
    "        depth = water_level.reshape(shape)\n",
    "        depth = np.vstack([depth, np.full((6, 320), 0)]) * mask\n",
    "        all_variables.append(depth)\n",
    "\n",
    "    # Extract source terms\n",
    "    source_variables = []\n",
    "    for data in data_list:\n",
    "        source = data[:, 4].reshape(shape)\n",
    "        source = source * dt / (dx * dy)\n",
    "        source = np.vstack([source, np.full((6, 320), 0)])\n",
    "        source_variables.append(source)\n",
    "\n",
    "    # Average consecutive source terms for smoother input\n",
    "    source_variables = [np.full((480, 320), 0)] + source_variables\n",
    "    source_variables1 = [(source_variables[i] + source_variables[i + 1]) / 2 for i in range(length)]\n",
    "    all_variables = all_variables + source_variables1 + [np.full((480, 320), 0)]\n",
    "\n",
    "    return all_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllVariablesDataset(Dataset):\n",
    "    def __init__(self, all_variables_list, roll_length=4, transform=Transform):\n",
    "        self.all_variables_list = [np.copy(item) for item in all_variables_list]\n",
    "        self.elevation = np.array(self.all_variables_list[0])\n",
    "        mid_index = (len(self.all_variables_list) - 1) // 2 + 1\n",
    "        self.depth = np.array(self.all_variables_list[1:mid_index])\n",
    "        self.source = np.array(self.all_variables_list[mid_index:])\n",
    "        self.roll_length = roll_length\n",
    "        self.transform = transform\n",
    "        self.dataset_length = len(self.depth) - self.roll_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Prepare model input: elevation + depth + source\n",
    "        seq_depth = [self.depth[idx]]\n",
    "        seq_source_1 = [self.source[idx]]\n",
    "        sequence = [self.elevation] + seq_depth + seq_source_1\n",
    "        sequence = np.stack(sequence, axis=0)\n",
    "        sequence = torch.tensor(sequence, dtype=torch.float32)\n",
    "\n",
    "        # Prepare targets and source for rolling prediction\n",
    "        target_roll = self.depth[idx+1:idx+1+self.roll_length]\n",
    "        target_roll = torch.tensor(target_roll, dtype=torch.float32)\n",
    "\n",
    "        source_roll = self.source[idx+1:idx+1+self.roll_length]\n",
    "        source_roll = torch.tensor(source_roll, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            seed = torch.randint(0, 2**32, (1,)).item()\n",
    "            torch.manual_seed(seed)\n",
    "            sequence = self.transform(sequence)\n",
    "            torch.manual_seed(seed)\n",
    "            source_roll = self.transform(source_roll)\n",
    "            torch.manual_seed(seed)\n",
    "            target_roll = self.transform(target_roll)\n",
    "\n",
    "        return sequence, source_roll, target_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets1 = []\n",
    "for folder_path in train_folder_paths:\n",
    "    all_variables = load_data_from_folder(folder_path)\n",
    "    datasets1.append(AllVariablesDataset(all_variables,transform=Transform)) \n",
    "combined_dataset = ConcatDataset(datasets1)\n",
    "datasets2 = []\n",
    "for folder_path in test_folder_paths:\n",
    "    all_variables = load_data_from_folder(folder_path)\n",
    "    datasets2.append(AllVariablesDataset(all_variables,transform=None)) \n",
    "test_dataset = ConcatDataset(datasets2)\n",
    "\n",
    "combined_dataloader = DataLoader(combined_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion=4, dilation=1, dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        expanded_channels = in_channels * expansion\n",
    "        self.stride = 1\n",
    "        \n",
    "        # Inverted residual block: expand -> depthwise -> project\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, expanded_channels, 1, bias=False),  # expand channels\n",
    "            nn.BatchNorm2d(expanded_channels, track_running_stats=False),\n",
    "            nn.ReLU6(),\n",
    "            \n",
    "            nn.Conv2d(expanded_channels, expanded_channels, 3, \n",
    "                      stride=1, padding=dilation, dilation=dilation,\n",
    "                      groups=expanded_channels, bias=False),  # depthwise conv with dilation\n",
    "            nn.BatchNorm2d(expanded_channels, track_running_stats=False),\n",
    "            nn.ReLU6(),\n",
    "            \n",
    "            nn.Conv2d(expanded_channels, out_channels, 1, bias=False),  # project channels\n",
    "            nn.BatchNorm2d(out_channels, track_running_stats=False)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels, track_running_stats=False)\n",
    "            )\n",
    "            \n",
    "        self.dropout = nn.Dropout2d(dropout_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x) + self.shortcut(x)\n",
    "        return self.dropout(F.relu(out))\n",
    "\n",
    "class LightSE(nn.Module):\n",
    "    \"\"\"Lightweight channel attention module\"\"\"\n",
    "    def __init__(self, channel, reduction=8):\n",
    "        super().__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel//reduction, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channel//reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        y = self.avgpool(x).view(b,c)\n",
    "        y = self.fc(y).view(b,c,1,1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class FloodUNet(nn.Module):\n",
    "    def __init__(self, dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        ch = [16, 32, 64, 128]  # channel configuration\n",
    "        \n",
    "        # Encoder (downsampling path)\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            MobileResidualBlock(3, ch[0], dilation=1),\n",
    "            MobileResidualBlock(ch[0], ch[0], dilation=2),\n",
    "            LightSE(ch[0])\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(ch[0], ch[0], 3, stride=2, padding=1)\n",
    "        \n",
    "        self.encoder2 = nn.Sequential(\n",
    "            MobileResidualBlock(ch[0], ch[1], dilation=1),\n",
    "            MobileResidualBlock(ch[1], ch[1], dilation=2),\n",
    "            LightSE(ch[1])\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(ch[1], ch[1], 3, stride=2, padding=1)\n",
    "        \n",
    "        self.encoder3 = nn.Sequential(\n",
    "            MobileResidualBlock(ch[1], ch[2], dilation=1),\n",
    "            MobileResidualBlock(ch[2], ch[2], dilation=2),\n",
    "            LightSE(ch[2])\n",
    "        )\n",
    "        self.down3 = nn.Conv2d(ch[2], ch[2], 3, stride=2, padding=1)\n",
    "        \n",
    "        # Bottleneck with large receptive field\n",
    "        self.bottom = nn.Sequential(\n",
    "            MobileResidualBlock(ch[2], ch[3], dilation=3),\n",
    "            MobileResidualBlock(ch[3], ch[3], dilation=5),\n",
    "            LightSE(ch[3])\n",
    "        )\n",
    "        \n",
    "        # Decoder (upsampling path)\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[3], ch[2], kernel_size=1)\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            MobileResidualBlock(ch[2]*2, ch[2]),\n",
    "            MobileResidualBlock(ch[2], ch[2])\n",
    "        )\n",
    "        \n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[2], ch[1], kernel_size=1)\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            MobileResidualBlock(ch[1]*2, ch[1]),\n",
    "            MobileResidualBlock(ch[1], ch[1])\n",
    "        )\n",
    "        \n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[1], ch[0], kernel_size=1)\n",
    "        )\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            MobileResidualBlock(ch[0]*2, ch[0]),\n",
    "            MobileResidualBlock(ch[0], ch[0])\n",
    "        )\n",
    "        \n",
    "        # Output head\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(ch[0], 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 1, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        e1 = self.encoder1(x)      # [16, 480,320]\n",
    "        d1 = self.down1(e1)        # [16, 240,160]\n",
    "        \n",
    "        e2 = self.encoder2(d1)     # [32, 240,160]\n",
    "        d2 = self.down2(e2)        # [32, 120,80]\n",
    "        \n",
    "        e3 = self.encoder3(d2)     # [64, 120,80]\n",
    "        d3 = self.down3(e3)        # [64, 60,40]\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottom(d3)        # [128, 60,40]\n",
    "        \n",
    "        # Decode\n",
    "        u3 = self.up3(b)           # [64, 120,80]\n",
    "        cat3 = torch.cat([e3, u3], dim=1)\n",
    "        dec3 = self.decoder3(cat3) # [64, 120,80]\n",
    "        \n",
    "        u2 = self.up2(dec3)        # [32, 240,160]\n",
    "        cat2 = torch.cat([e2, u2], dim=1)\n",
    "        dec2 = self.decoder2(cat2) # [32, 240,160]\n",
    "        \n",
    "        u1 = self.up1(dec2)        # [16, 480,320]\n",
    "        cat1 = torch.cat([e1, u1], dim=1)\n",
    "        dec1 = self.decoder1(cat1) # [16, 480,320]\n",
    "        \n",
    "        return self.output(dec1)   # [1, 480,320]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder (downsampling)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # downsample by 2\n",
    "            \n",
    "            nn.Conv2d(16, 32, 5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # downsample by 4\n",
    "            \n",
    "            nn.Conv2d(32, 64, 5, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # downsample by 8\n",
    "        )\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 2, stride=2),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 16, 2, stride=2),\n",
    "            nn.Conv2d(16, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(16, 1, 2, stride=2),\n",
    "            nn.ReLU()  # ensure non-negative output\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)\n",
    "\n",
    "# Basic UNet-like model\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Basic convolutional block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = ConvBlock(3, 16)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = ConvBlock(16, 32)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = ConvBlock(32, 64)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(64, 128)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec3 = ConvBlock(128, 64)\n",
    "        self.up2 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.dec2 = ConvBlock(64, 32)\n",
    "        self.up1 = nn.ConvTranspose2d(32, 16, 2, stride=2)\n",
    "        self.dec1 = ConvBlock(32, 16)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Conv2d(16, 1, 1)\n",
    "        self.final_act = nn.ReLU()  # ensure non-negative output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        e1 = self.enc1(x)                # [16, H, W]\n",
    "        e2 = self.enc2(self.pool1(e1))   # [32, H/2, W/2]\n",
    "        e3 = self.enc3(self.pool2(e2))   # [64, H/4, W/4]\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(self.pool3(e3))  # [128, H/8, W/8]\n",
    "        \n",
    "        # Decode\n",
    "        d3 = self.up3(b)                     # [64, H/4, W/4]\n",
    "        d3 = torch.cat([e3, d3], dim=1)      # [128, H/4, W/4]\n",
    "        d3 = self.dec3(d3)                   # [64, H/4, W/4]\n",
    "        \n",
    "        d2 = self.up2(d3)                    # [32, H/2, W/2]\n",
    "        d2 = torch.cat([e2, d2], dim=1)      # [64, H/2, W/2]\n",
    "        d2 = self.dec2(d2)                   # [32, H/2, W/2]\n",
    "        \n",
    "        d1 = self.up1(d2)                    # [16, H, W]\n",
    "        d1 = torch.cat([e1, d1], dim=1)      # [32, H, W]\n",
    "        d1 = self.dec1(d1)                   # [16, H, W]\n",
    "        \n",
    "        return self.final_act(self.output(d1))  # [1, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoResidualBlock(nn.Module):\n",
    "    \"\"\"Standard convolutional block without residuals\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm2d(out_channels, track_running_stats=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dropout = nn.Dropout2d(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.conv(x))\n",
    "\n",
    "class NoAttentionFloodUNet(nn.Module):\n",
    "    \"\"\"Ablation model: removes channel attention modules\"\"\"\n",
    "    def __init__(self, dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        ch = [16, 32, 64, 128]\n",
    "\n",
    "        # Encoder (LightSE removed)\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            MobileResidualBlock(3, ch[0], dilation=1),\n",
    "            MobileResidualBlock(ch[0], ch[0], dilation=2)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(ch[0], ch[0], 3, stride=2, padding=1)\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            MobileResidualBlock(ch[0], ch[1], dilation=1),\n",
    "            MobileResidualBlock(ch[1], ch[1], dilation=2)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(ch[1], ch[1], 3, stride=2, padding=1)\n",
    "\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            MobileResidualBlock(ch[1], ch[2], dilation=1),\n",
    "            MobileResidualBlock(ch[2], ch[2], dilation=2)\n",
    "        )\n",
    "        self.down3 = nn.Conv2d(ch[2], ch[2], 3, stride=2, padding=1)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottom = nn.Sequential(\n",
    "            MobileResidualBlock(ch[2], ch[3], dilation=3),\n",
    "            MobileResidualBlock(ch[3], ch[3], dilation=5)\n",
    "        )\n",
    "\n",
    "        # Decoder (same as FloodUNet)\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[3], ch[2], kernel_size=1)\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            MobileResidualBlock(ch[2]*2, ch[2]),\n",
    "            MobileResidualBlock(ch[2], ch[2])\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[2], ch[1], kernel_size=1)\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            MobileResidualBlock(ch[1]*2, ch[1]),\n",
    "            MobileResidualBlock(ch[1], ch[1])\n",
    "        )\n",
    "\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[1], ch[0], kernel_size=1)\n",
    "        )\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            MobileResidualBlock(ch[0]*2, ch[0]),\n",
    "            MobileResidualBlock(ch[0], ch[0])\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(ch[0], 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 1, kernel_size=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.encoder1(x)\n",
    "        d1 = self.down1(e1)\n",
    "        e2 = self.encoder2(d1)\n",
    "        d2 = self.down2(e2)\n",
    "        e3 = self.encoder3(d2)\n",
    "        d3 = self.down3(e3)\n",
    "        b = self.bottom(d3)\n",
    "        u3 = self.up3(b)\n",
    "        dec3 = self.decoder3(torch.cat([e3, u3], dim=1))\n",
    "        u2 = self.up2(dec3)\n",
    "        dec2 = self.decoder2(torch.cat([e2, u2], dim=1))\n",
    "        u1 = self.up1(dec2)\n",
    "        dec1 = self.decoder1(torch.cat([e1, u1], dim=1))\n",
    "        return self.output(dec1)\n",
    "\n",
    "\n",
    "class NoResidualFloodUNet(nn.Module):\n",
    "    \"\"\"Ablation model: removes residual connections, retains attention\"\"\"\n",
    "    def __init__(self, dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        ch = [16, 32, 64, 128]\n",
    "\n",
    "        # Encoder with attention (no residuals)\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, ch[0], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[0], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch[0], ch[0], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[0], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            LightSE(ch[0])\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(ch[0], ch[0], 3, stride=2, padding=1)\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(ch[0], ch[1], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[1], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch[1], ch[1], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[1], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            LightSE(ch[1])\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(ch[1], ch[1], 3, stride=2, padding=1)\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(ch[1], ch[2], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[2], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch[2], ch[2], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[2], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            LightSE(ch[2])\n",
    "        )\n",
    "        self.down3 = nn.Conv2d(ch[2], ch[2], 3, stride=2, padding=1)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottom = nn.Sequential(\n",
    "            nn.Conv2d(ch[2], ch[3], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[3], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch[3], ch[3], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[3], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            LightSE(ch[3])\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[3], ch[2], 1, bias=False)\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(ch[2]*2, ch[2], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[2], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch[2], ch[2], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[2], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[2], ch[1], 1, bias=False)\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(ch[1]*2, ch[1], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[1], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch[1], ch[1], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[1], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[1], ch[0], 1, bias=False)\n",
    "        )\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(ch[0]*2, ch[0], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[0], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch[0], ch[0], 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ch[0], track_running_stats=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Output\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(ch[0], 8, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(8, 1, 1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.dropout = nn.Dropout2d(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        d1 = self.down1(e1)\n",
    "        e2 = self.enc2(d1)\n",
    "        d2 = self.down2(e2)\n",
    "        e3 = self.enc3(d2)\n",
    "        d3 = self.down3(e3)\n",
    "        b = self.bottom(d3)\n",
    "        u3 = self.up3(b)\n",
    "        dec3 = self.dec3(torch.cat([e3, u3], dim=1))\n",
    "        u2 = self.up2(dec3)\n",
    "        dec2 = self.dec2(torch.cat([e2, u2], dim=1))\n",
    "        u1 = self.up1(dec2)\n",
    "        dec1 = self.dec1(torch.cat([e1, u1], dim=1))\n",
    "        out = self.output(dec1)\n",
    "        return self.dropout(out)\n",
    "\n",
    "    \n",
    "class PlainFloodUNet(nn.Module):\n",
    "    \"\"\"Ablation model: no residuals, no attention modules\"\"\"\n",
    "    def __init__(self, dropout_prob=0.0):\n",
    "        super().__init__()\n",
    "        ch = [16, 32, 64, 128]\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            NoResidualBlock(3, ch[0], dropout_prob=dropout_prob),\n",
    "            NoResidualBlock(ch[0], ch[0], dropout_prob=dropout_prob)\n",
    "        )\n",
    "        self.down1 = nn.Conv2d(ch[0], ch[0], kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            NoResidualBlock(ch[0], ch[1], dropout_prob=dropout_prob),\n",
    "            NoResidualBlock(ch[1], ch[1], dropout_prob=dropout_prob)\n",
    "        )\n",
    "        self.down2 = nn.Conv2d(ch[1], ch[1], kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            NoResidualBlock(ch[1], ch[2], dropout_prob=dropout_prob),\n",
    "            NoResidualBlock(ch[2], ch[2], dropout_prob=dropout_prob)\n",
    "        )\n",
    "        self.down3 = nn.Conv2d(ch[2], ch[2], kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottom = nn.Sequential(\n",
    "            NoResidualBlock(ch[2], ch[3], dropout_prob=dropout_prob),\n",
    "            NoResidualBlock(ch[3], ch[3], dropout_prob=dropout_prob)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[3], ch[2], kernel_size=1)\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            NoResidualBlock(ch[2]*2, ch[2], dropout_prob=dropout_prob),\n",
    "            NoResidualBlock(ch[2], ch[2], dropout_prob=dropout_prob)\n",
    "        )\n",
    "\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[2], ch[1], kernel_size=1)\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            NoResidualBlock(ch[1]*2, ch[1], dropout_prob=dropout_prob),\n",
    "            NoResidualBlock(ch[1], ch[1], dropout_prob=dropout_prob)\n",
    "        )\n",
    "\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(ch[1], ch[0], kernel_size=1)\n",
    "        )\n",
    "        self.dec1 = nn.Sequential(\n",
    "            NoResidualBlock(ch[0]*2, ch[0], dropout_prob=dropout_prob),\n",
    "            NoResidualBlock(ch[0], ch[0], dropout_prob=dropout_prob)\n",
    "        )\n",
    "\n",
    "        # Output\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(ch[0], 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(8, 1, kernel_size=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        d1 = self.down1(e1)\n",
    "        e2 = self.enc2(d1)\n",
    "        d2 = self.down2(e2)\n",
    "        e3 = self.enc3(d2)\n",
    "        d3 = self.down3(e3)\n",
    "        b = self.bottom(d3)\n",
    "        u3 = self.up3(b)\n",
    "        dec3 = self.dec3(torch.cat([e3, u3], dim=1))\n",
    "        u2 = self.up2(dec3)\n",
    "        dec2 = self.dec2(torch.cat([e2, u2], dim=1))\n",
    "        u1 = self.up1(dec2)\n",
    "        dec1 = self.dec1(torch.cat([e1, u1], dim=1))\n",
    "        return self.output(dec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloodLoss(nn.Module):\n",
    "    def __init__(self, mse_weight=1.0, flood_threshold=0.05, alpha=0, beta=0):\n",
    "        super().__init__()\n",
    "        self.mse_weight = mse_weight\n",
    "        self.flood_threshold = flood_threshold\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # Penalty for under-predicted flood area\n",
    "        flood_mask = targets > 0.1\n",
    "        under_pred_penalty = torch.clamp(0.1 - preds, min=0)\n",
    "        flood_penalty = (under_pred_penalty * flood_mask.float()).mean()\n",
    "\n",
    "        # Penalty for over-predicted non-flood area\n",
    "        unflood_mask = flood_mask <= 0.1\n",
    "        under_pred_penalty = torch.clamp(preds - 0.1, min=0)\n",
    "        flood_penalty += (under_pred_penalty * unflood_mask.float()).mean()\n",
    "\n",
    "        # Root mean square error\n",
    "        mse_loss = torch.mean(torch.abs(preds - targets) ** 2) ** 0.5\n",
    "\n",
    "        # Final loss weighted by flood coverage\n",
    "        denominator = (flood_mask.float().mean() ** 0.5)\n",
    "        total_loss = (self.mse_weight * mse_loss + self.alpha * flood_penalty) / denominator\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, scaler, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    time_steps = ROLL_LENGTH\n",
    "\n",
    "    # Apply exponential time-decay weighting\n",
    "    decay_rate = 0.8\n",
    "    time_indices = torch.arange(ROLL_LENGTH, device=device)\n",
    "    weights = torch.exp(-decay_rate * time_indices)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    for inputs, source_roll, target_roll in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        source_roll = source_roll.to(device)\n",
    "        target_roll = target_roll.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type='cuda', enabled=True):\n",
    "            current_input = inputs\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for step in range(time_steps):\n",
    "                outputs = model(current_input)\n",
    "                current_target = target_roll[:, step].unsqueeze(1)\n",
    "                step_loss = criterion(outputs, current_target)\n",
    "                total_loss += weights[step] * step_loss\n",
    "\n",
    "                if step < time_steps - 1:\n",
    "                    next_source = source_roll[:, step + 1].unsqueeze(1)\n",
    "                    current_input = torch.cat([\n",
    "                        inputs[:, 0:1],  # elevation\n",
    "                        outputs.detach(),  # predicted depth\n",
    "                        next_source       # next source input\n",
    "                    ], dim=1)\n",
    "\n",
    "            final_loss = total_loss / time_steps\n",
    "\n",
    "        scaler.scale(final_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += final_loss.item()\n",
    "\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    time_steps = ROLL_LENGTH\n",
    "    decay_rate = 0.8\n",
    "    time_indices = torch.arange(ROLL_LENGTH, device=device)\n",
    "    weights = torch.exp(-decay_rate * time_indices)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, source_roll, target_roll in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            source_roll = source_roll.to(device)\n",
    "            target_roll = target_roll.to(device)\n",
    "\n",
    "            current_input = inputs\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for step in range(time_steps):\n",
    "                outputs = model(current_input)\n",
    "                current_target = target_roll[:, step].unsqueeze(1)\n",
    "                step_loss = criterion(outputs, current_target)\n",
    "                total_loss += weights[step] * step_loss\n",
    "\n",
    "                if step < time_steps - 1:\n",
    "                    next_source = source_roll[:, step + 1].unsqueeze(1)\n",
    "                    current_input = torch.cat([\n",
    "                        inputs[:, 0:1],\n",
    "                        outputs,\n",
    "                        next_source\n",
    "                    ], dim=1)\n",
    "\n",
    "            val_loss += (total_loss / time_steps).item()\n",
    "\n",
    "    return val_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型和损失函数初始化\n",
    "model = PlainFloodUNet().to(device)\n",
    "criterion = FloodLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0002, weight_decay=5e-8)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "num_epochs = 200\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "patience = 200\n",
    "save_path = \"bestmodle.pth\"\n",
    "# 训练跟踪数据\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_epoch = 0\n",
    "\n",
    "model.to(device)\n",
    "with tqdm(total=num_epochs, desc='Training Progress', unit='epoch') as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        train_loss = train_one_epoch(model, combined_dataloader, \n",
    "                                   optimizer, criterion, scaler, device)\n",
    "        \n",
    "        # 验证阶段\n",
    "        val_loss = validate(model, test_dataloader, criterion, device)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss <= best_val_loss:\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'loss': val_loss\n",
    "            }, save_path)\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # 早停判断\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch}! Best epoch: {best_epoch}\")\n",
    "            break\n",
    "        \n",
    "        # 更新进度条\n",
    "        pbar.set_postfix({\n",
    "            'Train Loss': f'{train_loss:.4f}',\n",
    "            'Val Loss': f'{val_loss:.4f}',\n",
    "            'Best Val': f'{best_val_loss:.4f}',\n",
    "            'LR': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 实时可视化（每epoch刷新）\n",
    "        if epoch % 1 == 0 or epoch == num_epochs-1:\n",
    "            clear_output(wait=True)\n",
    "            pbar.update(1)\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            ax1 = plt.subplot(121)\n",
    "            ax1.semilogy(train_losses, label='Train', marker='.', alpha=0.8)\n",
    "            ax1.semilogy(val_losses, label='Val', marker='.', alpha=0.8)\n",
    "            ax1.scatter(best_epoch, best_val_loss, c='r', label=f'Best Val: {best_val_loss:.4f}')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend()\n",
    "            \n",
    "            ax2 = plt.subplot(122)\n",
    "            ax2.axhline(y=0.00005, color='g', linestyle='--', label='Fixed LR')\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.set_ylabel('Learning Rate')\n",
    "            ax2.set_title('Constant Learning Rate')\n",
    "            ax2.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
